{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset from Your Documentation\n",
    "\n",
    "![image](./imgs/‎GenAIEnterprises.‎012.png)\n",
    "\n",
    "![image](./imgs/‎GenAIEnterprises.‎013.png)\n",
    "\n",
    "![image](./imgs/‎GenAIEnterprises.‎014.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI\n",
    "from IPython.display import display, Markdown\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "def get_api_key(ssm_client, parameter_path):\n",
    "    '''Get the OpenAI API key from the SSM Parameter Store'''\n",
    "    try:\n",
    "        response = ssm_client.get_parameter(\n",
    "            Name=parameter_path,\n",
    "            WithDecryption=True\n",
    "        )\n",
    "        return response['Parameter']['Value']\n",
    "    except ssm_client.exceptions.ParameterNotFound:\n",
    "        raise Exception(f'Parameter {parameter_path} not found in SSM Parameter Store')\n",
    "\n",
    "# Create an SSM client using Boto3\n",
    "region_name = os.getenv('AWS_REGION', 'us-east-1') \n",
    "ssm = boto3.client('ssm', region_name=region_name)\n",
    "\n",
    "openai_api_key = get_api_key(ssm_client=ssm, parameter_path='/openai/api_key')\n",
    "langchain_api_key = get_api_key(ssm_client=ssm, parameter_path='/langchain/api_key')\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "os.environ['OPENAI_API_KEY'] = openai_api_key\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = langchain_api_key\n",
    "openai.api_key = openai_api_key\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "llm_model = \"gpt-3.5-turbo-16k\"\n",
    "\n",
    "# Create the vector store and embedding function\n",
    "embedding = OpenAIEmbeddings()\n",
    "vectordb = Chroma(\n",
    "    persist_directory='docs/chroma/',\n",
    "    embedding_function=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1351 docs\n",
      "Total length of all docs: 621737\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "\n",
    "required_exts = [\".md\"]\n",
    "\n",
    "reader = SimpleDirectoryReader(\n",
    "    input_dir=\"./handbook\", required_exts=required_exts, recursive=True\n",
    ")\n",
    "\n",
    "docs = reader.load_data()\n",
    "print(f\"Loaded {len(docs)} docs\")\n",
    "\n",
    "from llama_index import Document\n",
    "doc_text = \"\\n\\n\".join([d.get_content() for d in docs])\n",
    "print(f\"Total length of all docs: {len(doc_text)}\")\n",
    "\n",
    "metadata = {\"paper_title\": \"The Made Tech open source company handbook\"}\n",
    "docs = [Document(text=doc_text, metadata=metadata)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset\n",
    "\n",
    "Code from:\n",
    "\n",
    "https://gpt-index.readthedocs.io/en/latest/examples/finetuning/knowledge/finetune_knowledge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.callbacks import CallbackManager\n",
    "\n",
    "callback_manager = CallbackManager([])\n",
    "\n",
    "gpt_35_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-3.5-turbo-0613\", temperature=0.3),\n",
    "    callback_manager=callback_manager,\n",
    ")\n",
    "gpt_4_context = ServiceContext.from_defaults(\n",
    "    llm=OpenAI(model=\"gpt-4-0613\", temperature=0.3), callback_manager=callback_manager\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.evaluation import DatasetGenerator\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.indices.list import SummaryIndex\n",
    "\n",
    "# try evaluation modules\n",
    "from llama_index.evaluation import QueryResponseEvaluator, ResponseEvaluator\n",
    "from llama_index import PromptTemplate\n",
    "\n",
    "node_parser = SimpleNodeParser.from_defaults()\n",
    "nodes = node_parser.get_nodes_from_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node 0] Generated questions:\n",
      " ['What is the time frame within which employees at Made Tech will receive their rewards?', 'How soon will the TA team contact an individual after receiving a referral profile at Made Tech?', 'If the same candidate is referred by multiple employees at Made Tech, who will receive the reward?', 'What is the time limit for a referred candidate at Made Tech?', 'What happens to the reward if an employee leaves Made Tech?', 'Who is responsible for the IT infrastructure at Made Tech?', 'What are the standard laptop specifications for Engineers and UCD roles at Made Tech?', 'What are the standard laptop specifications for all other roles at Made Tech?', 'How often are laptops replaced at Made Tech?', 'What happens to the old laptops once they are returned at Made Tech?']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "157caba6b1ae4193987f67e8a4d9f418",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Node 0] Outputs: {'query': 'What is the time frame within which employees at Made Tech will receive their rewards?', 'response': 'Employees at Made Tech will receive their rewards within 30 days of each stage.'}\n",
      "[Node 0] Outputs: {'query': 'How soon will the TA team contact an individual after receiving a referral profile at Made Tech?', 'response': 'The TA team at Made Tech will contact an individual within 72 hours of receiving a referral profile.'}\n",
      "[Node 0] Outputs: {'query': 'If the same candidate is referred by multiple employees at Made Tech, who will receive the reward?', 'response': 'The first employee who refers the candidate will receive the reward.'}\n",
      "[Node 0] Outputs: {'query': 'What is the time limit for a referred candidate at Made Tech?', 'response': 'The time limit for a referred candidate at Made Tech is 12 months.'}\n",
      "[Node 0] Outputs: {'query': 'What happens to the reward if an employee leaves Made Tech?', 'response': 'The employee will not receive a payment if they leave Made Tech.'}\n",
      "[Node 0] Outputs: {'query': 'Who is responsible for the IT infrastructure at Made Tech?', 'response': 'Ops is responsible for the IT infrastructure at Made Tech.'}\n",
      "[Node 0] Outputs: {'query': 'What are the standard laptop specifications for Engineers and UCD roles at Made Tech?', 'response': 'The standard laptop specifications for Engineers and UCD roles at Made Tech are either a 14-inch MacBook Pro with Apple M1 Pro chip, 8-core CPU, 14-core GPU, 16-core Neural Engine, 16GB unified memory, and 512GB SSD storage, or a Lenovo ThinkPad X1 Carbon 14 Inch with Core i7-10510U Processor, 16GB of RAM, and 512GB SSD storage.'}\n",
      "[Node 0] Outputs: {'query': 'What are the standard laptop specifications for all other roles at Made Tech?', 'response': 'The standard laptop specifications for all other roles at Made Tech are as follows:\\n\\n- Macbook Air\\n- Apple M1 chip with 8‐core CPU, 7‐core GPU and 16‐core Neural Engine\\n- 8GB unified memory\\n- 256GB SSD storage1'}\n",
      "[Node 0] Outputs: {'query': 'How often are laptops replaced at Made Tech?', 'response': 'Laptops at Made Tech are replaced every 3 years, based on the age of the individual laptop, not the length of time a user has had it.'}\n",
      "[Node 0] Outputs: {'query': 'What happens to the old laptops once they are returned at Made Tech?', 'response': \"The old laptops at Made Tech are either traded in or recycled, depending on their condition. If a laptop is not eligible for trade-in, such as if it has an iCloud lock, is too damaged, not working, or has no value, it undergoes a full disk wipe and is recycled by Green Machine Computers. On the other hand, if a laptop has a trade-in value, it also undergoes a full disk wipe and is prepared to be in the best condition possible to maximize its trade-in valuation. This includes removing stickers, scrubbing off residue, and cleaning the device. The laptop is then valued through Stormfront's trade-in service and sent to a trade-in partner for analysis. Any payment received from the trade-in is credited to Made Tech's next invoice.\"}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "num_questions_per_chunk = 10\n",
    "question_gen_query = (\n",
    "    \"You are a Teacher/ Professor. Your task is to setup \"\n",
    "    \"a quiz/examination. Using the provided context, \"\n",
    "    f\"formulate {num_questions_per_chunk} that captures an important fact from the \"\n",
    "    \"context. \\n\"\n",
    "    \"You MUST obey the following criteria:\\n\"\n",
    "    \"- Restrict the question to the context information provided.\\n\"\n",
    "    \"- Do NOT create a question that cannot be answered from the context.\\n\"\n",
    "    \"- Phrase the question so that it does NOT refer to specific context. \"\n",
    "    'For instance, do NOT put phrases like \"given provided context\" or \"in this work\" in the question, '\n",
    "    \"because if the question is asked elsewhere it wouldn't be provided specific context. Replace these terms \"\n",
    "    \"with specific details.\\n\"\n",
    "    \"BAD questions:\\n\"\n",
    "    \"What did the author do in his childhood\\n\"\n",
    "    \"What were the main findings in this report\\n\\n\"\n",
    "    \"GOOD questions:\\n\"\n",
    "    \"What did Barack Obama do in his childhood\\n\"\n",
    "    \"What were the main findings in the original Transformers paper by Vaswani et al.\\n\\n\"\n",
    "    \"Generate the questions below:\\n\"\n",
    ")\n",
    "\n",
    "# go through each node one at a time -\n",
    "# generate questions, filter using eval modules, and dump to file\n",
    "\n",
    "fp = open(\"data/qa_pairs.jsonl\", \"a\")\n",
    "for idx, node in enumerate(nodes[30:31]):\n",
    "    dataset_generator = DatasetGenerator(\n",
    "        [node],\n",
    "        question_gen_query=question_gen_query,\n",
    "        service_context=gpt_4_context,\n",
    "        metadata_mode=\"all\",\n",
    "    )\n",
    "    node_questions_0 = dataset_generator.generate_questions_from_nodes(num=10)\n",
    "    print(f\"[Node {idx}] Generated questions:\\n {node_questions_0}\")\n",
    "    # for each question, get a response\n",
    "    for question in tqdm(node_questions_0):\n",
    "        index = SummaryIndex([node], service_context=gpt_35_context)\n",
    "        query_engine = index.as_query_engine()\n",
    "        response = query_engine.query(question)\n",
    "        out_dict = {\"query\": question, \"response\": str(response)}\n",
    "        print(f\"[Node {idx}] Outputs: {out_dict}\")\n",
    "        fp.write(json.dumps(out_dict) + \"\\n\")\n",
    "\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter out questions using QueryResponseEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try evaluation modules\n",
    "from llama_index.evaluation import QueryResponseEvaluator, ResponseEvaluator\n",
    "from llama_index import PromptTemplate\n",
    "from llama_index.llms import OpenAI\n",
    "\n",
    "\n",
    "query_eval_tmpl = PromptTemplate(\n",
    "    \"Your task is to evaluate the following: If the response for the query isn't able to answer the question provided.\\n\"\n",
    "    \"If query isn't able to answer the question, answer NO.\\n\"\n",
    "    \"Otherwise answer YES.\\n\"\n",
    "    \"To elaborate, you might get an answer like the following: 'The context does not contain the answer to this question.'\"\n",
    "    \"Please return NO in that case. \"\n",
    "    \"You be given the query and response. Return YES or NO as the answer.\\n\"\n",
    "    \"Query: \\n {query_str}\\n\"\n",
    "    \"Response: \\n {response_str}\\n\"\n",
    "    \"Answer: \"\n",
    ")\n",
    "\n",
    "eval_llm = OpenAI(model=\"gpt-4-0613\")\n",
    "\n",
    "def filter_data(path: str, out_path: str):\n",
    "    fp = open(path, \"r\")\n",
    "    out_fp = open(out_path, \"w\")\n",
    "    new_lines = []\n",
    "    for idx, line in enumerate(fp):\n",
    "        qa_pair = json.loads(line)\n",
    "        eval = eval_llm.complete(\n",
    "            query_eval_tmpl.format(\n",
    "                query_str=qa_pair[\"query\"], response_str=qa_pair[\"response\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        print(f\"[{idx}] QA Pair: {qa_pair} \\n Eval: {eval}\")\n",
    "        if \"NO\" in eval:\n",
    "            continue\n",
    "        else:\n",
    "            # new_lines.append(line)\n",
    "            out_fp.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] QA Pair: {'query': 'What is the mission of Made Tech as described in their handbook?', 'response': 'The mission of Made Tech, as described in their handbook, is to positively impact the future of the country by using technology to improve society. They work with public sector organizations to modernize technology and accelerate digital delivery so that citizens can benefit from better public services.'} \n",
      " Eval: YES\n",
      "[1] QA Pair: {'query': 'What is the purpose of the Made Tech Handbook for new team members?', 'response': 'The purpose of the Made Tech Handbook for new team members is to provide them with an overview of the company, including why it exists and its mission to positively impact society through technology. It also outlines the roles within the company, details the benefits and perks offered to employees, and provides information on staff welfare, team norms, and company processes, policies, and resources. Overall, the handbook serves as a starting point for new team members to familiarize themselves with Made Tech and its values.'} \n",
      " Eval: YES\n",
      "[2] QA Pair: {'query': 'How can individuals contribute to the Made Tech Handbook according to the handbook itself?', 'response': 'Individuals can contribute to the Made Tech Handbook by opening a pull request if they encounter something that should be included in the handbook or if they spot a typo. They can also open an issue if they have a question. This applies to both Made Tech employees and non-employees. For more information on contributing, including how to contribute as a non-technical user, individuals can refer to the CONTRIBUTING.md document.'} \n",
      " Eval: YES\n",
      "[3] QA Pair: {'query': 'What are some of the benefits and perks offered by Made Tech?', 'response': \"Made Tech offers several benefits and perks to its employees. Some of these include:\\n- 30 days' paid annual leave\\n- Flexible working hours and days\\n- Flexible parental leave options\\n- Hybrid working for all staff\\n- Paid counselling, as well as financial and legal advice\\n- Budget for work equipment\\n- Learning budget to develop skills\\n- Randomly matched lunch partners for new starters\\n- Regular social events\\n- Transparent salary bands\\n- Annual salary reviews\\n- Pension scheme with contributions\\n- Season ticket loan for travel expenses\\n- Cycle to Work scheme\\n- Expenses paid in line with policy\\n- Help to Buy Tech program for affordable technology purchases\\n- Income protection and life assurance for financial support when needed.\"} \n",
      " Eval: YES\n",
      "[4] QA Pair: {'query': 'What are the flexible working options provided by Made Tech?', 'response': 'Made Tech provides several flexible working options, including flexible working hours, flexible working days, and hybrid working for all staff.'} \n",
      " Eval: YES\n",
      "[5] QA Pair: {'query': 'What are some of the additional benefits offered by Made Tech to support work-life balance?', 'response': 'Made Tech offers several additional benefits to support work-life balance. These include:\\n- Taking Holiday: Made Tech provides 30 days of paid annual leave.\\n- Flexible Working Hours: They are flexible with the hours you work.\\n- Flexible Working Days: Made Tech is flexible with the number of days you work in a week.\\n- Flexible Parental Leave: They provide flexible parental leave options.\\n- Hybrid Working: Made Tech offers hybrid working for all of their staff.\\n- Paid counselling: They offer paid counselling, as well as financial and legal advice.'} \n",
      " Eval: YES\n",
      "[6] QA Pair: {'query': 'What are the provisions for fair compensation and benefits at Made Tech?', 'response': \"Made Tech provides several provisions for fair compensation and benefits. These include transparent salary bands, annual salary reviews, a pension scheme with employer contributions, a season ticket loan for travel expenses, a cycle to work scheme, expenses coverage in line with the company's policy, help to buy tech program, and income protection and life assurance for financial support during challenging times.\"} \n",
      " Eval: YES\n",
      "[7] QA Pair: {'query': 'What are the welfare measures mentioned in the Made Tech Handbook?', 'response': 'The welfare measures mentioned in the Made Tech Handbook include Sick Leave, State of Mind, Employee Assistance, Expectations between Employees and the Organisation, Raising an Issue, Parental Leave, DSE and H&S training, and Ethical Boundaries.'} \n",
      " Eval: YES\n",
      "[8] QA Pair: {'query': 'What are some of the resources and guides available for the team at Made Tech?', 'response': 'Some of the resources and guides available for the team at Made Tech include:\\n\\n- Team Norms\\n- Learning\\n- Mentorship\\n- Marketing Assets\\n- Line Management'} \n",
      " Eval: YES\n",
      "[9] QA Pair: {'query': 'What are the security policies in place at Made Tech as per their handbook?', 'response': 'The security policies in place at Made Tech, as per their handbook, include the Acceptable Usage Policy, BYOD Policy, Device Profiles, What To Do If Your Device Is Lost Or Stolen, Office Visitors, Last Day, and Password Policy.'} \n",
      " Eval: YES\n",
      "[10] QA Pair: {'query': 'What are the four policies mentioned in the Made Tech open source company handbook?', 'response': 'The four policies mentioned in the Made Tech open source company handbook are:\\n1. Anti-Corruption and Bribery Policy\\n2. Whistleblowing Policy\\n3. Anti-Slavery and Human Trafficking Policy\\n4. Dealing Code Policy'} \n",
      " Eval: YES\n",
      "[11] QA Pair: {'query': 'What is the name of the benefit provided by Made Tech that offers direct access to medical experts through an app?', 'response': 'The benefit provided by Made Tech that offers direct access to medical experts through an app is called \"Unum Help@hand.\"'} \n",
      " Eval: YES\n",
      "[12] QA Pair: {'query': 'What services does the Unum Help@hand benefit provide to Made Tech team members?', 'response': 'The Unum Help@hand benefit provides Made Tech team members with access to various services, including unlimited video consultations with UK based doctors, access to therapists through video consultations for mental health support, consultation and treatment with a network of physiotherapists for physiotherapy, access to UK based private consultants for medical second opinions, advice on a range of life and work issues through life, money, and wellbeing support, and a wellbeing calendar packed with resources, awareness dates, and support tools.'} \n",
      " Eval: YES\n",
      "[13] QA Pair: {'query': \"What is the purpose of Made Tech's participation in the Cycle to Work scheme?\", 'response': 'Made Tech\\'s participation in the Cycle to Work scheme is to support team members who wish to cycle to work for various reasons, such as promoting health, being environmentally friendly, or personal enjoyment. The company provides a limit of £2500 through CycleScheme, which can be used to purchase a bike and necessary accessories. Team members then make monthly \"salary sacrifice\" payments over a 12-month period to effectively \"hire\" the bike and kit. Once the payment period is completed, team members have options regarding owning the bike.'} \n",
      " Eval: YES\n",
      "[14] QA Pair: {'query': 'What is the maximum limit set by Made Tech for the CycleScheme?', 'response': 'The maximum limit set by Made Tech for the CycleScheme is £2500.'} \n",
      " Eval: YES\n",
      "[15] QA Pair: {'query': 'What is the process to get a bike through Cycle to Work on Made Tech Benefits Box?', 'response': 'To get a bike through Cycle to Work on Made Tech Benefits Box, you need to start your application through the CycleScheme portal accessible via Bob benefits. You will need the Employer Code, which can be found in the information section of the Bob benefits CycleScheme box. During the application process, you will use your Payroll number (labeled Employee ID on your bob profile or can be found on your payslip). Once you have completed the application, it will be sent to a member of the people team for approval. Once approved, you will receive an eCertificate that can be exchanged for your bike and gear either in-store or online. The deductions for the bike will begin the same month if you request your product before the 18th of the month. If you request your voucher on or after the 18th, the deductions will begin the following month.'} \n",
      " Eval: YES\n",
      "[16] QA Pair: {'query': 'Who is the provider of the CycleScheme as of 5th December 2022?', 'response': 'CycleScheme is the only provider of the Cycle to Work scheme as of 5th December 2022.'} \n",
      " Eval: YES\n",
      "[17] QA Pair: {'query': 'What is the recommended procedure before applying for the Cycle to Work scheme at Made Tech?', 'response': 'It is recommended to find your perfect bike and accessories before applying for the Cycle to Work scheme at Made Tech. This way, you will know exactly what amount to apply for and avoid applying for too much or too little. It is important to get this right because once you have been approved, you cannot amend your application.'} \n",
      " Eval: YES\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m filter_data(\u001b[39m\"\u001b[39;49m\u001b[39mdata/qa_pairs.jsonl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mdata/qa_pairs_2.jsonl\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[8], line 27\u001b[0m, in \u001b[0;36mfilter_data\u001b[0;34m(path, out_path)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mfor\u001b[39;00m idx, line \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(fp):\n\u001b[1;32m     26\u001b[0m     qa_pair \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mloads(line)\n\u001b[0;32m---> 27\u001b[0m     \u001b[39meval\u001b[39m \u001b[39m=\u001b[39m eval_llm\u001b[39m.\u001b[39;49mcomplete(\n\u001b[1;32m     28\u001b[0m         query_eval_tmpl\u001b[39m.\u001b[39;49mformat(\n\u001b[1;32m     29\u001b[0m             query_str\u001b[39m=\u001b[39;49mqa_pair[\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m], response_str\u001b[39m=\u001b[39;49mqa_pair[\u001b[39m\"\u001b[39;49m\u001b[39mresponse\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     30\u001b[0m         )\n\u001b[1;32m     31\u001b[0m     )\n\u001b[1;32m     33\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00midx\u001b[39m}\u001b[39;00m\u001b[39m] QA Pair: \u001b[39m\u001b[39m{\u001b[39;00mqa_pair\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m Eval: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39meval\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mNO\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39meval\u001b[39m:\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/llama_index/llms/base.py:277\u001b[0m, in \u001b[0;36mllm_completion_callback.<locals>.wrap.<locals>.wrapped_llm_predict\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mwith\u001b[39;00m wrapper_logic(_self) \u001b[39mas\u001b[39;00m callback_manager:\n\u001b[1;32m    268\u001b[0m     event_id \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_event_start(\n\u001b[1;32m    269\u001b[0m         CBEventType\u001b[39m.\u001b[39mLLM,\n\u001b[1;32m    270\u001b[0m         payload\u001b[39m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m         },\n\u001b[1;32m    275\u001b[0m     )\n\u001b[0;32m--> 277\u001b[0m     f_return_val \u001b[39m=\u001b[39m f(_self, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(f_return_val, Generator):\n\u001b[1;32m    279\u001b[0m         \u001b[39m# intercept the generator and add a callback to the end\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_gen\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponseGen:\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/llama_index/llms/openai.py:144\u001b[0m, in \u001b[0;36mOpenAI.complete\u001b[0;34m(self, prompt, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     complete_fn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_complete\n\u001b[0;32m--> 144\u001b[0m \u001b[39mreturn\u001b[39;00m complete_fn(prompt, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/llama_index/llms/generic_utils.py:153\u001b[0m, in \u001b[0;36mchat_to_completion_decorator.<locals>.wrapper\u001b[0;34m(prompt, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(prompt: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m CompletionResponse:\n\u001b[1;32m    151\u001b[0m     \u001b[39m# normalize input\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     messages \u001b[39m=\u001b[39m prompt_to_messages(prompt)\n\u001b[0;32m--> 153\u001b[0m     chat_response \u001b[39m=\u001b[39m func(messages, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    154\u001b[0m     \u001b[39m# normalize output\u001b[39;00m\n\u001b[1;32m    155\u001b[0m     \u001b[39mreturn\u001b[39;00m chat_response_to_completion_response(chat_response)\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/llama_index/llms/openai.py:194\u001b[0m, in \u001b[0;36mOpenAI._chat\u001b[0;34m(self, messages, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m message_dicts \u001b[39m=\u001b[39m to_openai_message_dicts(messages)\n\u001b[1;32m    193\u001b[0m all_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_all_kwargs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 194\u001b[0m response \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[1;32m    195\u001b[0m     is_chat_model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_is_chat_model,\n\u001b[1;32m    196\u001b[0m     max_retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    197\u001b[0m     messages\u001b[39m=\u001b[39;49mmessage_dicts,\n\u001b[1;32m    198\u001b[0m     stream\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    199\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mall_kwargs,\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    201\u001b[0m message_dict \u001b[39m=\u001b[39m response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    202\u001b[0m message \u001b[39m=\u001b[39m from_openai_message_dict(message_dict)\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/llama_index/llms/openai_utils.py:140\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(is_chat_model, max_retries, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     client \u001b[39m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[1;32m    138\u001b[0m     \u001b[39mreturn\u001b[39;00m client\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/llama_index/llms/openai_utils.py:138\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m    136\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    137\u001b[0m     client \u001b[39m=\u001b[39m get_completion_endpoint(is_chat_model)\n\u001b[0;32m--> 138\u001b[0m     \u001b[39mreturn\u001b[39;00m client\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/openai/api_resources/abstract/engine_api_resource.py:153\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    137\u001b[0m ):\n\u001b[1;32m    138\u001b[0m     (\n\u001b[1;32m    139\u001b[0m         deployment_id,\n\u001b[1;32m    140\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    154\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    155\u001b[0m         url,\n\u001b[1;32m    156\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    157\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    158\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    159\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    165\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/openai/api_requestor.py:288\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    278\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    279\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    287\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 288\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    289\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    290\u001b[0m         url,\n\u001b[1;32m    291\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    292\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    293\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    294\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    295\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    296\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    299\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/openai/api_requestor.py:596\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    594\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    595\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    597\u001b[0m         method,\n\u001b[1;32m    598\u001b[0m         abs_url,\n\u001b[1;32m    599\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    600\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    601\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    602\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    603\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    604\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/repositories/Compliance/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp\u001b[39m.\u001b[39mreadline(_MAXLINE \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    707\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1278\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1274\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1275\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1276\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1277\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1279\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1280\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.3/Frameworks/Python.framework/Versions/3.11/lib/python3.11/ssl.py:1134\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1133\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1136\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "filter_data(\"data/qa_pairs.jsonl\", \"data/qa_pairs_2.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "\n",
    "def split_train_val(path: str, out_train_path: str, out_val_path: str, train_split=0.7):\n",
    "    with open(path, \"r\") as fp:\n",
    "        lines = fp.readlines()\n",
    "\n",
    "        # shuffle the lines to make sure that the \"train questions\" cover most fo the context\n",
    "        shuffled_lines = deepcopy(lines)\n",
    "        random.shuffle(shuffled_lines)\n",
    "\n",
    "        split_idx = int(train_split * len(shuffled_lines))\n",
    "        train_lines = shuffled_lines[:split_idx]\n",
    "        val_lines = shuffled_lines[split_idx:]\n",
    "        with open(out_train_path, \"w\") as out_fp:\n",
    "            out_fp.write(\"\".join(train_lines))\n",
    "\n",
    "        with open(out_val_path, \"w\") as out_fp:\n",
    "            out_fp.write(\"\".join(val_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_train_val(\n",
    "    \"data/qa_pairs_2.jsonl\", \"data/qa_pairs_train.jsonl\", \"data/qa_pairs_val.jsonl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Format into Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = open(\"data/qa_pairs_train.jsonl\", \"r\")\n",
    "out_fp = open(\"data/qa_pairs_openai.jsonl\", \"w\")\n",
    "# TODO: try with different system prompts\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"Provide answers to questions based on the company handbook to help employees quickly find the information they need. Ensure that your responses are concise and directly address the questions asked without providing additional information.\",\n",
    "}\n",
    "for line in fp:\n",
    "    qa_pair = json.loads(line)\n",
    "    user_prompt = {\"role\": \"user\", \"content\": qa_pair[\"query\"]}\n",
    "    assistant_prompt = {\"role\": \"assistant\", \"content\": qa_pair[\"response\"]}\n",
    "    out_dict = {\n",
    "        \"messages\": [system_prompt, user_prompt, assistant_prompt],\n",
    "    }\n",
    "    out_fp.write(json.dumps(out_dict) + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
